{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_response=urlopen(\"https://onepiece.fandom.com/wiki/List_of_Canon_Characters\")\n",
    "response=raw_response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(response,\"html.parser\")\n",
    "table = soup.find(\"table\", class_=\"fandom-table sortable\")\n",
    "\n",
    "if table:\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    names={}\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) > 1: \n",
    "            name_link = cols[1].find(\"a\")\n",
    "            if name_link:\n",
    "                names[name_link.text] = name_link.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arc_list = [\n",
    "    \"Amazon Lily Arc\", \"Arabasta Arc\", \"Arabasta Arc\", \"Arlong Park Arc\", \"Arlong Park Arc\",\n",
    "    \"Baratie Arc\", \"Baratie Arc\", \"Dressrosa Arc\", \"Dressrosa Arc\", \"Drum Island Arc\", \n",
    "    \"Drum Island Arc\", \"Egghead Arc\", \"Egghead Arc\", \"Elbaph Arc\", \"Elbaph Arc\", \n",
    "    \"Enies Lobby Arc\", \"Enies Lobby Arc\", \"Fish-Man Island Arc\", \"Fish-Man Island Arc\",\n",
    "    \"Impel Down Arc\", \"Impel Down Arc\", \"Jaya Arc\", \"Jaya Arc\", \"Levely Arc\", \"Levely Arc\", \n",
    "    \"Little Garden Arc\", \"Little Garden Arc\", \"Loguetown Arc\", \"Loguetown Arc\", \n",
    "    \"Long Ring Long Land Arc\", \"Long Ring Long Land Arc\", \"Marineford Arc\", \"Marineford Arc\", \n",
    "    \"Orange Town Arc\", \"Orange Town Arc\", \"Post-Enies Lobby Arc\", \"Post-Enies Lobby Arc\", \n",
    "    \"Post-War Arc\", \"Post-War Arc\", \"Punk Hazard Arc\", \"Punk Hazard Arc\", \n",
    "    \"Return to Sabaody Arc\", \"Return to Sabaody Arc\", \"Reverse Mountain Arc\", \n",
    "    \"Reverse Mountain Arc\", \"Romance Dawn Arc\", \"Romance Dawn Arc\", \n",
    "    \"Sabaody Archipelago Arc\", \"Sabaody Archipelago Arc\", \"Skypiea Arc\", \"Skypiea Arc\", \n",
    "    \"Syrup Village Arc\", \"Syrup Village Arc\", \"Thriller Bark Arc\", \"Thriller Bark Arc\", \n",
    "    \"Wano Country Arc\", \"Wano Country Arc\", \"Water 7 Arc\", \"Water 7 Arc\", \n",
    "    \"Whisky Peak Arc\", \"Whisky Peak Arc\", \"Whole Cake Island Arc\", \"Whole Cake Island Arc\", \n",
    "    \"Zou Arc\", \"Zou Arc\"\n",
    "]\n",
    "\n",
    "\n",
    "unique_arcs = sorted(set(arc_list)) \n",
    "arc_dict = {i + 1: arc for i, arc in enumerate(unique_arcs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "# Initialize the dictionary of characters\n",
    "person_dict = names  # Example characters\n",
    "\n",
    "# Generate possible name variants (e.g., first name, last name) for each character\n",
    "def generate_name_variants(person_dict):\n",
    "    name_variants = {}\n",
    "    for full_name in person_dict.keys():\n",
    "        parts = full_name.split()  # Assuming the name is space-separated\n",
    "        name_variants[full_name] = parts  # Store the full name and its split parts\n",
    "    return name_variants\n",
    "\n",
    "name_variants = generate_name_variants(person_dict)\n",
    "\n",
    "# Define the folder containing HTML files\n",
    "html_folder = \"C:\\\\Users\\\\17675\\\\Desktop\\\\02805\\\\onepiece\"\n",
    "\n",
    "# Locate the specific <h2> tag whose child <span> contains \"History\"\n",
    "def find_history_section(soup):\n",
    "    for h2_tag in soup.find_all(\"h2\"):  # Loop through all <h2> tags\n",
    "        span = h2_tag.find(\"span\", class_=\"mw-headline\")  # Find the <span> tag\n",
    "        if span and span.text.strip() == \"History\":  # Check if the span text is \"History\"\n",
    "            return h2_tag  # Return the <h2> tag containing \"History\"\n",
    "    return None  # Return None if the target <h2> tag is not found\n",
    "\n",
    "# Prepare a dictionary to store arc relationship graphs\n",
    "arc_graph_map = {}\n",
    "arc_pattern = re.compile(r\".*Arc$\")  # Regular expression to match titles ending with \"Arc\"\n",
    "\n",
    "# Create a regular expression pattern for a full name (to avoid incorrect matches of middle parts)\n",
    "def create_full_name_pattern(full_name):\n",
    "    return re.compile(rf'\\b{re.escape(full_name)}\\b', re.IGNORECASE)\n",
    "\n",
    "# Create regular expression patterns for name variants\n",
    "def create_name_variant_patterns(variants):\n",
    "    return [re.compile(rf'\\b{re.escape(variant)}\\b', re.IGNORECASE) for variant in variants]\n",
    "\n",
    "empty_person = []  # List to track persons with no matching history section\n",
    "\n",
    "# Iterate through the HTML files\n",
    "for html_file in os.listdir(html_folder):\n",
    "    if html_file.endswith(\".txt\"):  # Only process text files\n",
    "        person_name = os.path.splitext(html_file)[0]  # The file name corresponds to the person's name\n",
    "        file_path = os.path.join(html_folder, html_file)\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()  # Read the file content\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        history_section = find_history_section(soup)\n",
    "\n",
    "        if not history_section:  # If no \"History\" section is found, skip the person\n",
    "            print(f\"No 'History' section for {person_name}\")\n",
    "            empty_person.append(person_name)\n",
    "            continue\n",
    "\n",
    "        # Get content from the \"History\" section until the next <h2>\n",
    "        current = history_section.find_next_sibling()\n",
    "        history_content = []\n",
    "\n",
    "        while current:\n",
    "            if current.name == \"h2\":  # Stop when another <h2> is found\n",
    "                break\n",
    "            history_content.append(current)\n",
    "            current = current.find_next_sibling()\n",
    "\n",
    "        # Track which names have been processed (to determine first appearances)\n",
    "        processed_names = set() \n",
    "\n",
    "        # Track fully matched full names\n",
    "        matched_full_names = set()  \n",
    "\n",
    "        for element in history_content:\n",
    "            if element.name in [\"h3\", \"h4\"]:  # Process only <h3> or <h4> tags\n",
    "                headline = element.find(\"span\", class_=\"mw-headline\")\n",
    "                if headline and arc_pattern.match(headline.text):  # Check if it matches an \"Arc\"\n",
    "                    current_arc = headline.text.strip()\n",
    "\n",
    "                    # Initialize a graph for the arc if not already present\n",
    "                    if current_arc not in arc_graph_map:\n",
    "                        arc_graph_map[current_arc] = nx.Graph()\n",
    "\n",
    "                    # Extract the arc content\n",
    "                    arc_content = []\n",
    "                    sibling = element.find_next_sibling()\n",
    "\n",
    "                    while sibling and not (sibling.name in [\"h3\", \"h4\", \"h2\"] and sibling.find(\"span\", class_=\"mw-headline\")):\n",
    "                        arc_content.append(sibling.text if sibling else \"\")\n",
    "                        sibling = sibling.find_next_sibling()\n",
    "\n",
    "                    # Join the content of the arc into a single string\n",
    "                    arc_text = \" \".join(arc_content)\n",
    "\n",
    "                    # Find all links in the HTML\n",
    "                    all_links = soup.find_all(\"a\", href=True)\n",
    "                    link_names = {link.get_text().strip() for link in all_links}\n",
    "\n",
    "                    for full_name, variants in name_variants.items():\n",
    "                        full_name_pattern = create_full_name_pattern(full_name)\n",
    "\n",
    "                        # Skip the person if their full name has not been processed and no link is found\n",
    "                        if full_name not in processed_names:\n",
    "                            if full_name in link_names:  # If the full name appears with a link\n",
    "                                processed_names.add(full_name)\n",
    "                            else:\n",
    "                                continue\n",
    "\n",
    "                        # Strict full name match\n",
    "                        if full_name_pattern.search(arc_text) and full_name != person_name and full_name in person_dict:\n",
    "                            matched_full_names.add(full_name)  # Mark the full name as matched\n",
    "                            arc_graph_map[current_arc].add_edge(person_name, full_name)\n",
    "\n",
    "                        # Match name variants if the full name has already been matched\n",
    "                        elif full_name in matched_full_names:\n",
    "                            for variant in variants:\n",
    "                                variant_pattern = create_name_variant_patterns([variant])[0]\n",
    "                                if variant_pattern.search(arc_text) and full_name != person_name:\n",
    "                                    arc_graph_map[current_arc].add_edge(person_name, full_name)\n",
    "                                    break\n",
    "\n",
    "        print(person_name)  # Print the current person's name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bartholomew Kuma\n",
      "Boa Hancock\n",
      "No 'Saga' section found for Boa Hancock at https://onepiece.fandom.com/wiki/Boa_Hancock/History.\n",
      "Brook\n",
      "Buggy\n",
      "Caesar Clown\n",
      "Charlotte Linlin\n",
      "Crocodile\n",
      "Donquixote Doflamingo\n",
      "Edward Newgate\n",
      "Franky\n",
      "Gaikotsu Yukichi\n",
      "No 'Saga' section found for Gaikotsu Yukichi at https://onepiece.fandom.com/wiki/Belly#Overview/History.\n",
      "Jack-in-the-Box\n",
      "No 'Saga' section found for Jack-in-the-Box at https://onepiece.fandom.com/wiki/Zombie#Jack-in-the-Box/History.\n",
      "Jew Wall\n",
      "Failed to fetch https://onepiece.fandom.com/wiki/Jew_Wall/History for Jew Wall: 404 Client Error: Not Found for url: https://onepiece.fandom.com/wiki/Jew_Wall/History\n",
      "Jinbe\n",
      "Kaidou\n",
      "Kakunoshin\n",
      "Failed to fetch https://onepiece.fandom.com/wiki/Kakunoshin/History for Kakunoshin: 404 Client Error: Not Found for url: https://onepiece.fandom.com/wiki/Kakunoshin/History\n",
      "Kin'emon\n",
      "No 'Saga' section found for Kin'emon at https://onepiece.fandom.com/wiki/Kin%27emon/History.\n",
      "Kumaguchi Ichiro\n",
      "No 'Saga' section found for Kumaguchi Ichiro at https://onepiece.fandom.com/wiki/Belly#Overview/History.\n",
      "Marshall D. Teach\n",
      "Mikio Itoo\n",
      "Failed to fetch https://onepiece.fandom.com/wiki/Mikio_Itoo/History for Mikio Itoo: 404 Client Error: Not Found for url: https://onepiece.fandom.com/wiki/Mikio_Itoo/History\n",
      "Monkey D. Luffy\n",
      "Mr. Sacrifice\n",
      "No 'Saga' section found for Mr. Sacrifice at https://onepiece.fandom.com/wiki/Sapoten_Graveyard#Buried_People/History.\n",
      "Nami\n",
      "Nefertari Vivi\n",
      "Nerona Imu\n",
      "Failed to fetch https://onepiece.fandom.com/wiki/Nerona_Imu/History for Nerona Imu: 404 Client Error: Not Found for url: https://onepiece.fandom.com/wiki/Nerona_Imu/History\n",
      "Nico Robin\n",
      "Pandawomanmi\n",
      "Failed to fetch https://onepiece.fandom.com/wiki/Pandawomanmi/History for Pandawomanmi: 404 Client Error: Not Found for url: https://onepiece.fandom.com/wiki/Pandawomanmi/History\n",
      "Portgas D. Ace\n",
      "Rob Lucci\n",
      "Roronoa Zoro\n",
      "Rug Bear\n",
      "No 'Saga' section found for Rug Bear at https://onepiece.fandom.com/wiki/Zombie#Rug_Bear/History.\n",
      "Sanji\n",
      "Shanks\n",
      "Smoker\n",
      "Tomato Gang\n",
      "Failed to fetch https://onepiece.fandom.com/wiki/Tomato_Gang/History for Tomato Gang: 404 Client Error: Not Found for url: https://onepiece.fandom.com/wiki/Tomato_Gang/History\n",
      "Tony Tony Chopper\n",
      "Trafalgar D. Water Law\n",
      "Usaguchi Hideo\n",
      "No 'Saga' section found for Usaguchi Hideo at https://onepiece.fandom.com/wiki/Belly#Overview/History.\n",
      "Usopp\n",
      "Wall Zombie\n",
      "No 'Saga' section found for Wall Zombie at https://onepiece.fandom.com/wiki/Zombie#Wall_Zombie/History.\n",
      "Willie Gallon\n",
      "Failed to fetch https://onepiece.fandom.com/wiki/Willie_Gallon/History for Willie Gallon: 404 Client Error: Not Found for url: https://onepiece.fandom.com/wiki/Willie_Gallon/History\n"
     ]
    }
   ],
   "source": [
    "import requests  # Import the requests library for making HTTP requests\n",
    "# Prepare to store new arc relationship graphs\n",
    "arc_pattern = re.compile(r\".*Arc$\")  # Regular expression to match titles ending with \"Arc\"\n",
    "sage_pattern = re.compile(r\".*Saga$\")  # Regular expression to match titles ending with \"Saga\"\n",
    "\n",
    "# Function to generate possible name variants (e.g., first name, last name) for each person\n",
    "def generate_name_variants(person_dict):\n",
    "    name_variants = {}\n",
    "    for full_name in person_dict.keys():\n",
    "        parts = full_name.split()  # Assume that names are split by spaces\n",
    "        name_variants[full_name] = parts  # Store full names and their component parts\n",
    "    return name_variants\n",
    "\n",
    "name_variants = generate_name_variants(names)  # names is your existing dictionary\n",
    "\n",
    "# Set to store names that have been processed to avoid duplication\n",
    "processed_names = set()\n",
    "\n",
    "# Function to check if a text represents a person's name, considering links and first-time occurrences\n",
    "def is_person_name(text, link_tag):\n",
    "    # If it's the first occurrence and no link is found, it's not considered a name\n",
    "    if text not in processed_names:\n",
    "        processed_names.add(text)\n",
    "        # Skip if no link exists\n",
    "        if not link_tag:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Function to generate a regular expression pattern for matching full names (to avoid partial matches)\n",
    "def create_full_name_pattern(full_name):\n",
    "    return re.compile(rf'\\b{re.escape(full_name)}\\b', re.IGNORECASE)\n",
    "\n",
    "# Function to create regular expression patterns for name variants\n",
    "def create_name_variant_patterns(variants):\n",
    "    return [re.compile(rf'\\b{re.escape(variant)}\\b', re.IGNORECASE) for variant in variants]\n",
    "\n",
    "# Loop through the names that are not found previously (empty_person)\n",
    "for person_name in empty_person:\n",
    "    print(person_name)\n",
    "    if person_name not in names:\n",
    "        print(f\"URL for {person_name} not found in names dictionary.\")\n",
    "        continue\n",
    "\n",
    "    # Get the new URL for the person's history section\n",
    "    original_url = names[person_name]\n",
    "    history_url = f\"https://onepiece.fandom.com{original_url}/History\"\n",
    "\n",
    "    try:\n",
    "        # Make a request to fetch the page content\n",
    "        response = requests.get(history_url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find all h2 tags, and filter those with titles ending in \"Saga\"\n",
    "        h2_tags = soup.find_all(\"h2\")\n",
    "        sage_h2s = [h2 for h2 in h2_tags if h2.find(\"span\", class_=\"mw-headline\") and sage_pattern.match(h2.find(\"span\", class_=\"mw-headline\").text.strip())]\n",
    "\n",
    "        # Skip if no matching \"Saga\" section is found\n",
    "        if not sage_h2s:\n",
    "            print(f\"No 'Saga' section found for {person_name} at {history_url}.\")\n",
    "            continue\n",
    "        \n",
    "        # Iterate through the found \"Saga\" titles\n",
    "        for arc_start in sage_h2s:\n",
    "\n",
    "            current = arc_start.find_next_sibling()\n",
    "            in_arc_section = True  # Enter a valid \"Arc\" section\n",
    "\n",
    "            while current:\n",
    "                if current.name == \"h2\":  # When a new h2 title is encountered\n",
    "                    next_span = current.find(\"span\", class_=\"mw-headline\")\n",
    "                    if next_span and not sage_pattern.match(next_span.text.strip()):\n",
    "                        in_arc_section = False  # Leave the \"Arc\" section\n",
    "                        break\n",
    "\n",
    "                elif current.name == \"h3\" and in_arc_section:  # Check if it's an \"Arc\" title\n",
    "                    headline = current.find(\"span\", class_=\"mw-headline\")\n",
    "                    if headline and arc_pattern.match(headline.text.strip()):\n",
    "                        current_arc = headline.text.strip()\n",
    "\n",
    "                        # Initialize the graph for the current arc if it doesn't exist\n",
    "                        if current_arc not in arc_graph_map:\n",
    "                            arc_graph_map[current_arc] = nx.Graph()\n",
    "\n",
    "                        # Extract the content of the arc\n",
    "                        arc_text = []\n",
    "                        sibling = current.find_next_sibling()\n",
    "                        \n",
    "                        # Iterate through content until the next \"Saga\" or \"Arc\" title is found\n",
    "                        while sibling and not (\n",
    "                            (sibling.name == \"h2\" and sibling.find(\"span\", class_=\"mw-headline\") and sage_pattern.match(\n",
    "                                sibling.find(\"span\", class_=\"mw-headline\").text.strip()))  # New 'Saga' section found\n",
    "                            or (sibling.name == \"h3\" and sibling.find(\"span\", class_=\"mw-headline\") and arc_pattern.match(\n",
    "                                sibling.find(\"span\", class_=\"mw-headline\").text.strip()))  # New \"Arc\" section found\n",
    "                            or (sibling.name == \"h2\" and sibling.find(\"span\", class_=\"mw-headline\") and not sage_pattern.match(\n",
    "                                sibling.find(\"span\", class_=\"mw-headline\").text.strip()))  # Non-\"Saga\" h2 title found\n",
    "                        ):\n",
    "                            arc_text.append(sibling.text if sibling else \"\")\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "                        # Join arc content into a single string\n",
    "                        arc_text = \" \".join(arc_text)\n",
    "\n",
    "                        # Find all links and extract their names\n",
    "                        all_links = soup.find_all(\"a\", href=True)\n",
    "                        link_names = {link.get_text().strip() for link in all_links}\n",
    "\n",
    "                        # Find all matching person names\n",
    "                        matched_full_names = set()  # Track fully matched names\n",
    "\n",
    "                        for full_name, variants in name_variants.items():\n",
    "                            full_name_pattern = create_full_name_pattern(full_name)\n",
    "\n",
    "                            # Skip if the person's name is first encountered without a link\n",
    "                            if full_name not in processed_names:\n",
    "                                if full_name in link_names:  # If the name appears with a link\n",
    "                                    processed_names.add(full_name)\n",
    "                                else:\n",
    "                                    continue\n",
    "\n",
    "                            # Strict full name match\n",
    "                            if full_name_pattern.search(arc_text) and full_name != person_name and full_name in names:\n",
    "                                matched_full_names.add(full_name)  # Mark as matched\n",
    "                                arc_graph_map[current_arc].add_edge(person_name, full_name)\n",
    "\n",
    "                            # Match variants only if the full name has already matched\n",
    "                            elif full_name in matched_full_names:\n",
    "                                for variant in variants:\n",
    "                                    variant_pattern = create_name_variant_patterns([variant])[0]\n",
    "                                    if variant_pattern.search(arc_text) and full_name != person_name:\n",
    "                                        arc_graph_map[current_arc].add_edge(person_name, full_name)\n",
    "                                        break\n",
    "\n",
    "                current = current.find_next_sibling()\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch {history_url} for {person_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marineford Arc with 165 nodes and 623 edges\n",
      "Post-War Arc with 166 nodes and 261 edges\n",
      "Zou Arc with 127 nodes and 419 edges\n",
      "Dressrosa Arc with 188 nodes and 1185 edges\n",
      "Thriller Bark Arc with 78 nodes and 350 edges\n",
      "Wano Country Arc with 439 nodes and 2279 edges\n",
      "Fish-Man Island Arc with 94 nodes and 433 edges\n",
      "Levely Arc with 131 nodes and 352 edges\n",
      "Egghead Arc with 311 nodes and 1057 edges\n",
      "Skypiea Arc with 65 nodes and 261 edges\n",
      "Impel Down Arc with 76 nodes and 241 edges\n",
      "Little Garden Arc with 37 nodes and 102 edges\n",
      "Drum Island Arc with 34 nodes and 103 edges\n",
      "Arabasta Arc with 82 nodes and 347 edges\n",
      "Whole Cake Island Arc with 162 nodes and 869 edges\n",
      "Return to Sabaody Arc with 50 nodes and 116 edges\n",
      "Punk Hazard Arc with 73 nodes and 285 edges\n",
      "Romance Dawn Arc with 32 nodes and 65 edges\n",
      "Loguetown Arc with 55 nodes and 121 edges\n",
      "Jaya Arc with 75 nodes and 210 edges\n",
      "Amazon Lily Arc with 58 nodes and 138 edges\n",
      "Enies Lobby Arc with 85 nodes and 412 edges\n",
      "Sabaody Archipelago Arc with 99 nodes and 384 edges\n",
      "Arlong Park Arc with 39 nodes and 175 edges\n",
      "Post Enies Lobby Arc with 26 nodes and 34 edges\n",
      "Elbaph Arc with 48 nodes and 82 edges\n",
      "Whisky Peak Arc with 36 nodes and 92 edges\n",
      "Baratie Arc with 34 nodes and 115 edges\n",
      "Long Ring Long Land Arc with 45 nodes and 124 edges\n",
      "Water 7 Arc with 69 nodes and 308 edges\n",
      "Orange Town Arc with 24 nodes and 52 edges\n",
      "Syrup Village Arc with 29 nodes and 90 edges\n",
      "Reverse Mountain Arc with 19 nodes and 59 edges\n",
      "Post-Enies Lobby Arc with 105 nodes and 199 edges\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "# Standard arc names list\n",
    "standard_arc_names = [\n",
    "    \"Amazon Lily Arc\", \"Arabasta Arc\", \"Arlong Park Arc\", \"Baratie Arc\", \"Dressrosa Arc\", \n",
    "    \"Drum Island Arc\", \"Egghead Arc\", \"Elbaph Arc\", \"Enies Lobby Arc\", \"Fish-Man Island Arc\",\n",
    "    \"Impel Down Arc\", \"Jaya Arc\", \"Levely Arc\", \"Little Garden Arc\", \"Loguetown Arc\", \n",
    "    \"Long Ring Long Land Arc\", \"Marineford Arc\", \"Orange Town Arc\", \"Post-Enies Lobby Arc\", \n",
    "    \"Post-War Arc\", \"Punk Hazard Arc\", \"Return to Sabaody Arc\", \"Reverse Mountain Arc\", \n",
    "    \"Romance Dawn Arc\", \"Sabaody Archipelago Arc\", \"Skypiea Arc\", \"Syrup Village Arc\", \n",
    "    \"Thriller Bark Arc\", \"Wano Country Arc\", \"Water 7 Arc\", \"Whisky Peak Arc\", \"Whole Cake Island Arc\", \"Zou Arc\"\n",
    "]\n",
    "\n",
    "# Create a dictionary mapping normalized names to the standard names\n",
    "normalized_name_map = {name.replace(\" \", \"\").replace(\"-\", \"\").lower(): name for name in standard_arc_names}\n",
    "\n",
    "# Function to normalize arc names (remove spaces and hyphens, and convert to lowercase)\n",
    "def normalize_arc_name(name):\n",
    "    return name.replace(\" \", \"\").replace(\"-\", \"\").lower()\n",
    "\n",
    "# Merge similar arc graphs based on the normalized arc names\n",
    "def merge_similar_arcs(arc_graph_map, normalized_name_map):\n",
    "    to_delete = []  # List to store arcs that need to be deleted\n",
    "    merged_graphs = {}  # Dictionary to store merged arc graphs\n",
    "\n",
    "    for arc_name in arc_graph_map:\n",
    "        # Normalize the arc name\n",
    "        normalized_name = normalize_arc_name(arc_name)\n",
    "\n",
    "        # Find the reference name corresponding to the normalized name\n",
    "        reference_name = normalized_name_map.get(normalized_name, None)\n",
    "\n",
    "        if reference_name:\n",
    "            # If the normalized name matches a standard name, merge the graph\n",
    "            if reference_name in merged_graphs:\n",
    "                # Merge the current graph into the existing graph\n",
    "                merged_graphs[reference_name] = nx.compose(merged_graphs[reference_name], arc_graph_map[arc_name])\n",
    "                to_delete.append(arc_name)\n",
    "            else:\n",
    "                # If it's the first time encountering the standard name, add the graph\n",
    "                merged_graphs[reference_name] = arc_graph_map[arc_name]\n",
    "        else:\n",
    "            # If no match is found for the standard name, delete the arc graph\n",
    "            to_delete.append(arc_name)\n",
    "\n",
    "    # Delete the unnecessary arc graphs\n",
    "    for arc_name in to_delete:\n",
    "        del arc_graph_map[arc_name]\n",
    "\n",
    "    # Add the merged graphs to the arc_graph_map\n",
    "    arc_graph_map.update(merged_graphs)\n",
    "\n",
    "# Call the function to merge similar arcs\n",
    "merge_similar_arcs(arc_graph_map, normalized_name_map)\n",
    "\n",
    "# Print the merged results\n",
    "for arc_name, graph in arc_graph_map.items():\n",
    "    print(f\"{arc_name} with {len(graph.nodes)} nodes and {len(graph.edges)} edges\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
