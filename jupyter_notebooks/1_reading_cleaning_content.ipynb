{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import networkx as nx\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.request import urlopen\n",
    "import networkx as nx\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get character's names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_response=urlopen(\"https://onepiece.fandom.com/wiki/List_of_Canon_Characters\")\n",
    "response=raw_response.read()\n",
    "soup=BeautifulSoup(response,\"html.parser\")\n",
    "table = soup.find(\"table\", class_=\"fandom-table sortable\")\n",
    "\n",
    "if table:\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    names={}\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) > 1: \n",
    "            name_link = cols[1].find(\"a\")\n",
    "            if name_link:\n",
    "                names[name_link.text] = name_link.get(\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to download and save the character's contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://onepiece.fandom.com\"\n",
    "\n",
    "output_folder = \"../onepiece\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "timeout_links = {}\n",
    "for name, relative_link in names.items():\n",
    "    full_url = urljoin(base_url, relative_link)\n",
    "    try:\n",
    "        response = requests.get(full_url,timeout=2)\n",
    "        response.raise_for_status()\n",
    "        file_path = os.path.join(output_folder, f\"{name}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "\n",
    "        print(f\"Saved {name} page to {file_path}\")\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        timeout_links[name] = full_url\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read all file contents into a dictionary\n",
    "def load_files_to_memory(file_paths):\n",
    "    content_dict = {}\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content_dict[os.path.splitext(os.path.basename(file_path))[0]] = file.read()\n",
    "    return content_dict\n",
    "\n",
    "# Function to process one file and find relationships\n",
    "def process_file(person_name, file_content, all_names):\n",
    "    edges = []\n",
    "    for other_person in all_names:\n",
    "        if other_person != person_name and other_person in file_content:\n",
    "            edges.append((person_name, other_person))\n",
    "    return edges\n",
    "\n",
    "# Get file paths and names\n",
    "file_paths = [os.path.join(r'./../onepiece', f) for f in os.listdir(os.path.join(r'./../onepiece'))]\n",
    "file_names = [os.path.splitext(os.path.basename(f))[0] for f in file_paths]\n",
    "\n",
    "# Load all file contents into memory\n",
    "file_contents = load_files_to_memory(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to clean and save html content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_content(html_content):\n",
    "    \"\"\"\n",
    "    Extracts and cleans the main text content from the OnePiece Wiki HTML page.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): The raw HTML content of the page.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and filtered text content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Locate the main content container\n",
    "    content_div = soup.find('div', class_='mw-parser-output')\n",
    "    if not content_div:\n",
    "        return \"Main content not found.\"\n",
    "    \n",
    "    # Remove unwanted elements like scripts, styles, asides, and the TOC (table of contents)\n",
    "    for unwanted in content_div(['script', 'style', 'table', 'aside']):\n",
    "        unwanted.extract()\n",
    "    \n",
    "    # Remove the \"Contents\" section explicitly\n",
    "    toc_div = soup.find('div', {'id': 'toc', 'class': 'toc'})\n",
    "    if toc_div:\n",
    "        toc_div.extract()\n",
    "    \n",
    "    # Remove specific unwanted sections (e.g., Gallery, Trivia, References, etc.)\n",
    "    unwanted_sections = [\"Video Games\", \"Playable Appearance\", \"Gallery\", \"Merchandise\",\"Translation and Dub Issues\", \"Trivia\", \"References\", \"Site Navigation\"]\n",
    "    for header in content_div.find_all(['h2', 'h3', 'h4']):  # Headers indicate sections\n",
    "        if any(section in header.get_text() for section in unwanted_sections):\n",
    "            # Remove header and its following sibling content\n",
    "            for sibling in header.find_next_siblings():\n",
    "                if sibling.name in ['h2', 'h3', 'h4']:  # Stop at next section\n",
    "                    break\n",
    "                sibling.extract()\n",
    "            header.extract()\n",
    "\n",
    "    # Get cleaned text content\n",
    "    text_content = content_div.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    # Remove bracketed references like [1], [6.0], [28], etc.\n",
    "    text_content = re.sub(r'\\[\\d+(\\.\\d+)?\\]', '', text_content)\n",
    "\n",
    "    # Remove empty lines and normalize spacing\n",
    "    text_content = \"\\n\".join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_content(html_contents, output_folder):\n",
    "    \"\"\"\n",
    "    Cleans and saves the extracted content from a dictionary of HTML files.\n",
    "\n",
    "    Args:\n",
    "        html_contents (dict): A dictionary with filenames as keys and HTML content as values.\n",
    "        output_folder (str): Path to the folder where cleaned files will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)  \n",
    "    \n",
    "    for file_name, html_content in html_contents.items():\n",
    "        # Clean the content\n",
    "        cleaned_content = extract_clean_content(html_content)\n",
    "        \n",
    "        # Create the output file path\n",
    "        file_path = os.path.join(output_folder, f\"{file_name}.txt\")\n",
    "        \n",
    "        # Save the cleaned content\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(cleaned_content)\n",
    "\n",
    "output_folder = r\"./../onepiece_cleaned\"  \n",
    "save_cleaned_content(file_contents, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
