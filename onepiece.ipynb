{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.request import urlopen\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_response=urlopen(\"https://onepiece.fandom.com/wiki/List_of_Canon_Characters\")\n",
    "response=raw_response.read()\n",
    "soup=BeautifulSoup(response,\"html.parser\")\n",
    "table = soup.find(\"table\", class_=\"fandom-table sortable\")\n",
    "\n",
    "if table:\n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    names={}\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) > 1: \n",
    "            name_link = cols[1].find(\"a\")\n",
    "            if name_link:\n",
    "                names[name_link.text] = name_link.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base_url = \"https://onepiece.fandom.com\"\n",
    "\n",
    "output_folder = \"C:\\\\Users\\\\17675\\\\Desktop\\\\02450\\\\onepiece\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "timeout_links = {}\n",
    "for name, relative_link in names.items():\n",
    "    full_url = urljoin(base_url, relative_link)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(full_url,timeout=2)\n",
    "        response.raise_for_status()\n",
    "        file_path = os.path.join(output_folder, f\"{name}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "\n",
    "        print(f\"Saved {name} page to {file_path}\")\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        timeout_links[name] = full_url\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1486\n",
      "Number of edges: 289633\n"
     ]
    }
   ],
   "source": [
    "# Function to read all file contents into a dictionary\n",
    "def load_files_to_memory(file_paths):\n",
    "    content_dict = {}\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content_dict[os.path.splitext(os.path.basename(file_path))[0]] = file.read()\n",
    "    return content_dict\n",
    "\n",
    "# Function to process one file and find relationships\n",
    "def process_file(person_name, file_content, all_names):\n",
    "    edges = []\n",
    "    for other_person in all_names:\n",
    "        if other_person != person_name and other_person in file_content:\n",
    "            edges.append((person_name, other_person))\n",
    "    return edges\n",
    "\n",
    "# Get file paths and names\n",
    "file_paths = [os.path.join('onepiece', f) for f in os.listdir(os.path.join('onepiece'))]\n",
    "file_names = [os.path.splitext(os.path.basename(f))[0] for f in file_paths]\n",
    "\n",
    "# Load all file contents into memory\n",
    "file_contents = load_files_to_memory(file_paths)\n",
    "\n",
    "# Initialize a graph\n",
    "graph = nx.Graph()\n",
    "graph.add_nodes_from(file_names)  # Add all nodes\n",
    "\n",
    "# Use multithreading to find edges\n",
    "edges = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(process_file, person_name, content, file_names)\n",
    "        for person_name, content in file_contents.items()\n",
    "    ]\n",
    "    for future in futures:\n",
    "        edges.extend(future.result())\n",
    "\n",
    "# Add edges to the graph\n",
    "graph.add_edges_from(edges)\n",
    "\n",
    "# Summary of the graph\n",
    "print(\"Number of nodes:\", graph.number_of_nodes())\n",
    "print(\"Number of edges:\", graph.number_of_edges())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities: 6\n",
      "Community sizes: [257, 246, 167, 184, 246, 386]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform Louvain community detection\n",
    "communities = louvain_communities(graph)\n",
    "\n",
    "# Assign community labels as attributes to nodes\n",
    "community_mapping = {}\n",
    "for community_id, community_nodes in enumerate(communities):\n",
    "    for node in community_nodes:\n",
    "        community_mapping[node] = community_id\n",
    "\n",
    "# Add community attribute to each node\n",
    "nx.set_node_attributes(graph, community_mapping, \"community\")\n",
    "\n",
    "# Export the graph with community data\n",
    "output_path = \"onepiece_person_relationships.gexf\"\n",
    "nx.write_gexf(graph, output_path)\n",
    "\n",
    "# Print summary of detected communities\n",
    "print(\"Number of communities:\", len(communities))\n",
    "print(\"Community sizes:\", [len(c) for c in communities])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import networkx as nx\n",
    "# from bs4 import BeautifulSoup\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# # Function to read and parse file content\n",
    "# def extract_titles_from_file(file_path):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         content = file.read()\n",
    "#         soup = BeautifulSoup(content, 'html.parser')\n",
    "#         return [tag['title'] for tag in soup.find_all('a', title=True)]\n",
    "\n",
    "# # Function to find relationships based on extracted titles\n",
    "# def process_file_with_titles(person_name, file_titles, all_titles):\n",
    "#     edges = []\n",
    "#     for other_person in all_titles:\n",
    "#         if other_person != person_name and other_person in file_titles:\n",
    "#             edges.append((person_name, other_person))\n",
    "#     return edges\n",
    "\n",
    "# # Get file paths and names\n",
    "# file_paths = [os.path.join('onepiece', f) for f in os.listdir(os.path.join('onepiece'))]\n",
    "# file_names = [os.path.splitext(os.path.basename(f))[0] for f in file_paths]\n",
    "\n",
    "# # Extract titles from all files\n",
    "# titles_dict = {}\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     futures = {executor.submit(extract_titles_from_file, file_path): file_name for file_path, file_name in zip(file_paths, file_names)}\n",
    "#     for future in futures:\n",
    "#         file_name = futures[future]\n",
    "#         titles_dict[file_name] = future.result()\n",
    "\n",
    "# # Initialize a graph\n",
    "# graph = nx.Graph()\n",
    "# graph.add_nodes_from(file_names)  # Add all nodes\n",
    "\n",
    "# # Use multithreading to find edges\n",
    "# edges = []\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     futures = [\n",
    "#         executor.submit(process_file_with_titles, person_name, titles, set(titles_dict.keys()))\n",
    "#         for person_name, titles in titles_dict.items()\n",
    "#     ]\n",
    "#     for future in futures:\n",
    "#         edges.extend(future.result())\n",
    "\n",
    "# # Add edges to the graph\n",
    "# graph.add_edges_from(edges)\n",
    "\n",
    "# # Summary of the graph\n",
    "# print(\"Number of nodes:\", graph.number_of_nodes())\n",
    "# print(\"Number of edges:\", graph.number_of_edges())\n",
    "\n",
    "# # Export the graph for visualization\n",
    "# nx.write_gexf(graph, \"person_relationships_onepiece.gexf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Epithet', 'Monkey D. Luffy', 'Roronoa Zoro', 'Swordsman', 'Nami', 'Navigator', 'Usopp', 'Sniper', 'Sanji', 'Cook', 'Tony Tony Chopper', 'Doctor', 'Nico Robin', 'Archaeologist', 'Franky', 'Shipwright', 'Brook', 'Musician', 'Jinbe', 'Helmsman', 'Nefertari Vivi']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
